getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000000,2)
getmode(X)
X<-rexp(1000000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(1000,2)
getmode(X)
X<-rexp(10000000000000,2)
getmode(X)
X<-rexp(10000000,2)
getmode(X)
for(n in 1:10) {
r<-rdunif(10000,1,n)
median(r)
getmode(r)
}
install.packages("purrr")
for(n in 1:10) {
r<-rdunif(10000,1,n)
median(r)
getmode(r)
}
library(purrr)
for(n in 1:10) {
r<-rdunif(10000,1,n)
median(r)
getmode(r)
}
for(n in 1:10) {
r<-rdunif(10000,1,n)
print(median(r))
print(getmode(r))
}
for(n in 1:10) {
r<-rdunif(10000,1,n)
print(n+","+median(r))
print(getmode(r))
}
for(n in 1:10) {
r<-rdunif(10000,1,n)
print(median(r))
print(getmode(r))
}
for(n in 1:10) {
r<-rdunif(10000,1,n)
print(n)
print(median(r))
print(getmode(r))
print()
}
for(n in 1:10) {
r<-rdunif(10000,1,n)
print(n)
print(median(r))
print(getmode(r))
print("---------------------------")
}
for(n in 1:10) {
r<-rdunif(10000,1,n)
print(---------------------------)
print(n)
print(median(r))
print(getmode(r))
print(---------------------------)
}
for(n in 1:10) {
r<-rdunif(10000,1,n)
print(n)
print(median(r))
print(getmode(r))
print("---------------------------")
}
for(n in 1:10) {
r<-rdunif(10000,1,n)
print(n)
print(median(r))
print(getmode(r))
print("---------------------------")
}
for(n in 1:10) {
r<-rdunif(10000,1,n)
print(getmode(r))
print("---------------------------")
}
for(n in 1:10) {
r<-rdunif(10000,1,n)
print(median(r))
}
for(n in 1:10) {
r<-rdunif(10000,1,n)
print(median(r))
}
for(n in 1:10) {
r<-rdunif(10000,1,n)
print(median(r))
}
X=runif(10000,0,1)
Y=runif(10000,0,1)
print(cov(X+Y))
print(cov(X-Y))
X=runif(10000,0,1)
Y=runif(10000,0,1)
print(cov(X+Y,X-Y))
X=runif(10000,0,1)
Y=runif(10000,0,1)
print(cov(X+Y,X-Y))
X=runif(10000,0,1)
Y=runif(10000,0,1)
print(cov(X+Y,X-Y))
X=runif(10000,0,1)
Y=runif(10000,0,1)
print(cov(X+Y,X-Y))
X=runif(10000,0,1)
Y=runif(10000,0,1)
print(cov(X+Y,X-Y))
X=runif(10000,0,1)
Y=runif(10000,0,1)
print(cov(X+Y,X-Y))
X=runif(10000,0,1)
Y=runif(10000,0,1)
print(cov(X+Y,X-Y))
X=runif(10000,0,1)
Y=runif(10000,0,1)
print(cov(X+Y,X-Y))
X=runif(10000,0,1)
Y=runif(10000,0,1)
print(cov(X+Y,X-Y))
y=matrix(c(0.018,0.035,0.031,0.008,0.018,
0.002,0.112,0.064,0.032,0.069,
0.001,0.066,0.094,0.032,0.084,
0.001,0.018,0.019,0.010,0.051,
0.001,0.029,0.032,0.043,0.130),nrow=5,byrow=TRUE)
<<<<<<< HEAD
colnames(y)=c("farm","operatives","craftsmen","sales","professional")
=======
colnames(y)=c(”farm”,”operatives”,”craftsmen”,”sales”,”professional”)
>>>>>>> origin/main
rownames(y)=colnames(y)
#makesurethisisajointdistribution
sum(y)
#thereturnedvalueisindeed1
y=matrix(c(0.018,0.035,0.031,0.008,0.018,
0.002,0.112,0.064,0.032,0.069,
0.001,0.066,0.094,0.032,0.084,
0.001,0.018,0.019,0.010,0.051,
0.001,0.029,0.032,0.043,0.130),nrow=5,byrow=TRUE)
colnames(y)=c("farm","operatives","craftsmen","sales","professional")
rownames(y)=colnames(y)
#makesurethisisajointdistribution
sum(y)
#thereturnedvalueisindeed1
sum(c("farm"))
c("farm")
y("farm")
colSums(y)
rowSums(y)
colSums(y)/0.11
dd=y/0.11
dd
rowSums(dd)
y["farm"]
y[farm]
y['farm']
y['farm','farm']
y['farm',]
y['farm',]/rowSum(y[,'farm'])
y['farm',]
y['farm',]/SUM(y['farm',])
y['farm',]
y['farm',]/0.11
y['farm',]/rowSums(y)['farm']
y[,'farm']/colSums(y)['farm']
y[,'farm']/0.023
N <- rpois(1000, 700)
X <- rpois(1000, 800000)
mean(N * X)
700*800000
N <- rpois(1, 700)
X <- rpois(1, 800000)
var(N * X)
N <- rpois(100, 700)
X <- rpois(100, 800000)
mean(N)*mean(x)
N <- rpois(100, 700)
X <- rpois(100, 800000)
mean(N)*mean(X)
mean(X)*mean(X)*var(N)+mean(N)*var(X)
install.packages("rmarkdown")
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
tinytex::install_tinytex()
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
install.packages(c("htmltools", "magrittr", "rlang"))
install.packages(c("htmltools", "magrittr", "rlang"))
install.packages(c("htmltools", "magrittr", "rlang"))
install.packages(c("htmltools", "magrittr", "rlang"))
install.packages(c("htmltools", "magrittr", "rlang"))
pnorm(1.58)
1-pnorm(1.58)
pnorm(-1.58)
2*pnorm(-abs((15-20)/sqrt(10)))
sum(dbinom(c(0:44,56:100),size=100,p=0.5))
sum(dbinom(c(0:44,56:100),size=100,p=0.5))
sum(dbinom(c(0:44,56:100),size=100,p=0.5))
sum(dbinom(c(0:44,56:100),size=100,p=0.5))
sum(dbinom(c(0:44,56:100),size=100,p=0.5))
sum(dbinom(c(0:44,56:100),size=100,p=0.5))
sum(dbinom(c(0:15,25:40),size=40,p=0.5))
2*pnorm(-abs((305-280)/sqrt(84)))
sum(dbinom(c(0:305,95:400),size=400,p=0.7))
sum(dbinom(c(0:305,365:400),size=400,p=0.7))
sum(dbinom(c(0:305,365:400),size=400,p=0.7))
sum(dbinom(c(0:14,26:40),size=40,p=0.5))
sum(dbinom(c(0:305,365:400),size=400,p=0.7))
sum(dbinom(c(0:95,305:400),size=400,p=0.7))
sum(dbinom(c(0:95,305:400),size=400,p=0.3))
sum(dbinom(c(0:262,298:400),size=400,p=0.8))
sum(dbinom(c(0:0.72*400,0.8*400:400),size=400,p=0.8))
sum(dbinom(c(0:0.72*400,0.8*400:400),size=400,p=0.7))
sum(dbinom(c(0:280,305:400),size=400,p=0.7))
sum(dbinom(c(0:255,305:400),size=400,p=0.7))
sum(dbinom(c(0:0.72*400,0.8*400:400),size=400,p=0.7))
pbinom(0.72*400,size=400,p=0.7)
.72*400
sum(dbinom(c(0:262,298:400),size=400,p=0.7))
sum(dbinom(c(0:255, 305:400), size = 400, p =0.7))
sum(dbinom(c(0:255, 305:400), size = 400, p =0.7))
sum(dbinom(c(0:255, 305:400), size = 400, p =0.7))
sum(dbinom(c(0:255, 305:400), size = 400, p =0.7))
sum(dbinom(c(0:255, 305:400), size = 400, p =0.7))
sum(dbinom(c(0:255, 305:400), size = 400, p =0.7))
sum(dbinom(c(0:255, 305:400), size = 400, p =0.7))
pbinom(280-25, size=400, prob=0.7)+1-pbinom(280+25, size=400, prob=0.7)
sum(dbinom(c(0:254, 306:400), size = 400, p =0.7))
sum(dbinom(c(0:254, 306:400), size = 400, p=0.7))
sum(dbinom(c(0:255, 305:400), size = 400, p=0.7))
rep(1:3,rep(6,3))
rep(1:3,rep(10,3))
rep(1:3,rep(6,2))
rep(1:5,rep(6,3))
rep(1:5,rep(6,5))
setwd("C:\Users\edwar\OneDrive - UW\DATA 557\Final Project\data_557")
setwd("C://Users//edwar//OneDrive - UW//DATA 557//Final Project//data_557")
getwd()
world_bank_data = read.csv("./raw_data/world_bank_population_gdp.csv")
world_bank_data
names(world_bank_data)
library(tidyverse)
library(lubridate)
temp = pivot_wider(world_bank_data, names_from = "Series.Name", values_from = "X2019..YR2019.")
world_bank_data %>%
pivot_wider(world_bank_data, names_from = "Series.Name", values_from = "X2019..YR2019.")
world_bank_data %>%
pivot_wider(world_bank_data, names_from = "Series.Name", values_from = "X2019..YR2019.")
world_bank_data %>%
pivot_wider(names_from = "Series.Name", values_from = "X2019..YR2019.")
world_bank_data %>%
pivot_wider(names_from = "Series.Name", values_from = "X2019..YR2019.")
temp =
world_bank_data %>%
pivot_wider(names_from = Series.Name, values_from = X2019..YR2019.)
world_bank_data %>%
pivot_wider(names_from = Series.Name, values_from = X2019..YR2019.)
world_bank_gdp = read.csv("./raw_data/world_bank_gdp.csv")
# Main pipeline that runs all data prep and analysis steps.
# You will need to be in the root of the repository for this to work!
stopifnot(grepl("data_557", getwd()))
# First take the "raw" data, add country code, and save to "intermediates" folder.
source('add_country_code.R')
# Calculate the number of cases per country after one month, two months, six months
source('First case.R')
# Then, take this intermediates data, clean and merge it, and save to "prepped" folder.
source('prep_data.R')
# Main pipeline that runs all data prep and analysis steps.
# You will need to be in the root of the repository for this to work!
stopifnot(grepl("data_557", getwd()))
# First take the "raw" data, add country code, and save to "intermediates" folder.
source('add_country_code.R')
# Calculate the number of cases per country after one month, two months, six months
source('First case.R')
# Then, take this intermediates data, clean and merge it, and save to "prepped" folder.
source('prep_data.R')
source('add_country_code.R')
source('add_country_code.R')
# Main pipeline that runs all data prep and analysis steps.
# You will need to be in the root of the repository for this to work!
stopifnot(grepl("data_557", getwd()))
# First take the "raw" data, add country code, and save to "intermediates" folder.
source('add_country_code.R')
# Calculate the number of cases per country after one month, two months, six months
source('First case.R')
# Then, take this intermediates data, clean and merge it, and save to "prepped" folder.
source('prep_data.R')
# Main pipeline that runs all data prep and analysis steps.
# You will need to be in the root of the repository for this to work!
stopifnot(grepl("data_557", getwd()))
# First take the "raw" data, add country code, and save to "intermediates" folder.
source('add_country_code.R')
# Calculate the number of cases per country after one month, two months, six months
source('First case.R')
# Then, take this intermediates data, clean and merge it, and save to "prepped" folder.
source('prep_data.R')
sixmonth_data = read.csv("./prepped_data/sixmonth.csv")
sixmonth_data
names(sixmonth_data)
# import libraries
library(tidyverse)
library(lubridate)
library(ggpubr)
# load sixmonth data
sixmonth_data = read.csv("./prepped_data/sixmonth.csv")
# check correlation between gdp_pc and overall GHSI index
ggscatter(sixmonth_data,x=gdp_pc ,y=overall,
add='reg.line',cor.coef=TRUE,cor.method='pearson',
xlab='GDP per-capita',ylab='Overall GHSI Score')
names(sixmonth_data)
# import libraries
library(tidyverse)
library(lubridate)
library(ggpubr)
# load sixmonth data
sixmonth_data = read.csv("./prepped_data/sixmonth.csv")
# check correlation between gdp_pc and overall GHSI index
ggscatter(sixmonth_data,x='gdp_pc' ,y='overall',
add='reg.line',cor.coef=TRUE,cor.method='pearson',
xlab='GDP per-capita',ylab='Overall GHSI Score')
# import libraries
library(tidyverse)
library(lubridate)
library(ggpubr)
# load sixmonth data
sixmonth_data = read.csv("./prepped_data/sixmonth.csv")
# check correlation between gdp_pc and overall GHSI index
ggscatter(sixmonth_data,x='gdp_pc' ,y='overall',
add='reg.line',cor.coef=TRUE,cor.method='pearson',
xlab='GDP per-capita',ylab='Overall GHSI Score')
# import libraries
library(tidyverse)
library(lubridate)
library(ggpubr)
# load sixmonth data
sixmonth_data = na.omit(read.csv("./prepped_data/sixmonth.csv"))
# check correlation between gdp_pc and overall GHSI index
ggscatter(sixmonth_data,x='gdp_pc' ,y='overall',
add='reg.line',cor.coef=TRUE,cor.method='pearson',
xlab='GDP per-capita',ylab='Overall GHSI Score')
# import libraries
library(tidyverse)
library(lubridate)
library(ggpubr)
# load sixmonth data
sixmonth_data = na.omit(read.csv("./prepped_data/sixmonth.csv"))
# check correlation between gdp_pc and overall GHSI index
ggscatter(sixmonth_data,x='gdp_pc' ,y='overall',
add='reg.line',cor.coef=TRUE,cor.method='pearson',
xlab='GDP per-capita',ylab='Overall GHSI Score')
quantile(sixmonth_data$gdp_pc, c(0.25,0.5,0.75,1))
quantile(sixmonth_data$gdp_pc, c(0.25,0.5,0.75,1))[1]
quantile(sixmonth_data$gdp_pc, c(0.25,0.5,0.75,1))[2]
quantile(sixmonth_data$gdp_pc, c(0.25,0.5,0.75,1))[3]
quantile(sixmonth_data$gdp_pc, c(0.25,0.5,0.75,1))[4]
sixmonth_data$quartile <- ntile(sixmonth_data$overall, 4)
names(sixmonth_data)
sixmonth_data
head(sixmonth_data)
head(sixmonth_data)["gdp_pd","quartile"]
quantile(sixmonth_data$gdp_pc, c(0.25,0.5,0.75,1))
sixmonth_data$quartile <- ntile(sixmonth_data$gdp_pc, 4)
head(sixmonth_data)
# import libraries
library(tidyverse)
library(lubridate)
library(ggpubr)
library(dplyr)
# load sixmonth data
sixmonth_data = na.omit(read.csv("./prepped_data/sixmonth.csv"))
# check correlation between gdp_pc and overall GHSI index
ggscatter(sixmonth_data,x='gdp_pc' ,y='overall',
add='reg.line',cor.coef=TRUE,cor.method='pearson',
xlab='GDP per-capita',ylab='Overall GHSI Score')
# add gdp_pc quartile
sixmonth_data$quartile <- ntile(sixmonth_data$gdp_pc, 4)
plot_scatter(
data = sixmonth_data,
x = sixmonth_data$overall,
y = sixmonth_data$casepc,
grp = sixmonth_data$quartile
)
library(ggplot)
# import libraries
library(tidyverse)
library(lubridate)
library(ggpubr)
library(ggplot)
library(dplyr)
# load sixmonth data
sixmonth_data = na.omit(read.csv("./prepped_data/sixmonth.csv"))
# check correlation between gdp_pc and overall GHSI index
ggscatter(sixmonth_data,x='gdp_pc' ,y='overall',
add='reg.line',cor.coef=TRUE,cor.method='pearson',
xlab='GDP per-capita',ylab='Overall GHSI Score')
# add gdp_pc quartile
sixmonth_data$quartile <- ntile(sixmonth_data$gdp_pc, 4)
plot_scatter(
data = sixmonth_data,
x = sixmonth_data$overall,
y = sixmonth_data$casepc,
grp = sixmonth_data$quartile
)
# import libraries
library(tidyverse)
library(lubridate)
library(ggpubr)
library(ggplot2)
library(dplyr)
# load sixmonth data
sixmonth_data = na.omit(read.csv("./prepped_data/sixmonth.csv"))
# check correlation between gdp_pc and overall GHSI index
ggscatter(sixmonth_data,x='gdp_pc' ,y='overall',
add='reg.line',cor.coef=TRUE,cor.method='pearson',
xlab='GDP per-capita',ylab='Overall GHSI Score')
# add gdp_pc quartile
sixmonth_data$quartile <- ntile(sixmonth_data$gdp_pc, 4)
plot_scatter(
data = sixmonth_data,
x = sixmonth_data$overall,
y = sixmonth_data$casepc,
grp = sixmonth_data$quartile
)
librart(ggplot)
librar(ggplot)
library(ggplot)
library(ggplot2)
plot_scatter
()
plot_scatter()
sixmonth_data %>% group_by(quartile) %>% tally()
# import libraries
# load sixmonth data
sixmonth_data = na.omit(read.csv("./prepped_data/sixmonth.csv"))
# check correlation between gdp_pc and overall GHSI index
ggscatter(sixmonth_data,x='gdp_pc' ,y='overall',
add='reg.line',cor.coef=TRUE,cor.method='pearson',
xlab='GDP per-capita',ylab='Overall GHSI Score')
# add gdp_pc quartile
sixmonth_data$gdp_pc_quartile <- ntile(sixmonth_data$gdp_pc, 4)
summary(lm(casepc ~ factor(gdp_pc_quartile), data = sixmonth_data))
names(sixmonth_data)
summary(lm(casepc ~ factor(gdp_pc_quartile), data = sixmonth_data))
summary(lm(deathpc ~ factor(gdp_pc_quartile), data = sixmonth_data))
summary(lm(cfratio ~ factor(gdp_pc_quartile), data = sixmonth_data))
summary(lm(casepc ~ factor(gdp_pc_quartile), data = sixmonth_data))$coefficients
summary(lm(deathpc ~ factor(gdp_pc_quartile), data = sixmonth_data))$coefficients
summary(lm(cfratio ~ factor(gdp_pc_quartile), data = sixmonth_data))$coefficients
summary(lm(casepc ~ overall + factor(gdp_pc_quartile), data = sixmonth_data))$coefficients
summary(lm(deathpc ~ overall + factor(gdp_pc_quartile), data = sixmonth_data))$coefficients
summary(lm(cfratio ~ overall + factor(gdp_pc_quartile), data = sixmonth_data))$coefficients
summary(lm(casepc ~ overall + factor(gdp_pc_quartile), data = sixmonth_data))$coefficients
plot(lm(casepc ~ overall + factor(gdp_pc_quartile), data = sixmonth_data))
# Main pipeline that runs all data prep and analysis steps.
# You will need to be in the root of the repository for this to work!
stopifnot(grepl("data_557", getwd()))
# First take the "raw" data, add country code, and save to "intermediates" folder.
source('add_country_code.R')
# Calculate the number of cases per country after one month, two months, six months
source('First case.R')
# Then, take this intermediates data, clean and merge it, and save to "prepped" folder.
source('prep_data.R')
# Main pipeline that runs all data prep and analysis steps.
# You will need to be in the root of the repository for this to work!
stopifnot(grepl("data_557", getwd()))
# First take the "raw" data, add country code, and save to "intermediates" folder.
source('add_country_code.R')
# Calculate the number of cases per country after one month, two months, six months, twelve months
source('First case.R')
# Then, take this intermediates data, clean and merge it, and save to "prepped" folder.
source('prep_data.R')
# Main pipeline that runs all data prep and analysis steps.
# You will need to be in the root of the repository for this to work!
stopifnot(grepl("data_557", getwd()))
# First take the "raw" data, add country code, and save to "intermediates" folder.
source('add_country_code.R')
# Calculate the number of cases per country after one month, two months, six months, twelve months
source('First case.R')
# Then, take this intermediates data, clean and merge it, and save to "prepped" folder.
<<<<<<< HEAD
source('prep_data.R')
=======
source('prep_data.R')
>>>>>>> origin/main
