---
title: "normalize"
author: "Alison Gale"
date: "2/27/2021"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(scales)
```

## Testing strategies for normalizing coefficients

Start by loading the six month data:

```{r}
data <- read.csv(file = '../prepped_data/six_month_outlier_screened.csv')
```

As a baseline, run a regression on the overall GHSI score:

```{r}
old_fit = lm(formula = cfratio ~ overall, data = data)
summary(old_fit)

plot(data$overall, data$cfratio)
```

Suppose we run the following regression to get coefficients for our model:

```{r}
fit = lm(formula = cfratio ~ prev_emergence_pathogens + early_detection + rapid_response + robust_health_sector + commitments + risk_environment, data = data)
summary(fit)
```

Now we retrieve estimated values for the data:

```{r}
estimates = fitted.values(fit)
```

And we normalize these values from 0 to 100:

```{r}
norm = rescale(estimates, c(100, 0))
```

Add this back to the data frame:

```{r}
data$new_overall = norm
```

Finally, we run a new regression:

```{r}
new_fit = lm(formula = cfratio ~ new_overall, data = data)
summary(new_fit)
```

Plot the data:

```{r}
plot(data$new_overall, data$cfratio)
```

In summary, we can get the correlation to go in the correct direction with the
new score and the plots of the data look a little better. The improvement was
more noticeable for cases-per-capita. Additionally the statistical
significance of the intercept is higher. We might be able to improve this by
adding in our confounding variables (possibly to replace the sub-components with
weights closer to zero).